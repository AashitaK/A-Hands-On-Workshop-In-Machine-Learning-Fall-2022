{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Hands-on Workshop series in Machine Learning\n",
    "### Session 4: More on Neural Networks\n",
    "#### Instructor: Aashita Kesarwani\n",
    "\n",
    "What is Deep Learning?\n",
    "\n",
    "Why the sudden growth in the applications of Deep Learning?   \n",
    "1. Data availability\n",
    "2. Computing power \n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*GTzatEUd4cICPVub.\" width=\"600\" height=\"850\" />\n",
    "\n",
    "\n",
    "\n",
    "#### Revision:\n",
    "\n",
    "Let us summarize what we have learned so far.\n",
    "\n",
    "Terminology:\n",
    "* Nodes\n",
    "* Input layer\n",
    "* Hidden layers\n",
    "* Output layer\n",
    "* Weights\n",
    "* Bias\n",
    "* Weighted sum $z_i$\n",
    "* Activation functions $g_i$\n",
    "* Sigmoid function (also known as logistic function)\n",
    "* Activations $a_i$\n",
    "* Cost function $J$\n",
    "* Logloss function (also known as cross-entropy function)\n",
    "* Gradient descent algorithm\n",
    "* Forward propagation\n",
    "* Backward propagation\n",
    "* Iterations\n",
    "* Epochs\n",
    "* Learning rate $\\alpha$ \n",
    "* Overfitting and underfitting\n",
    "* Multi-layer Perceptron (MLP)\n",
    "* Binary classification, positive and negative classes, decision boundary\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Mohamed_Zahran6/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png\" width=\"400\" height=\"450\" />\n",
    "<p style=\"text-align: center;\"> Multi-layer Perceptron </p>\n",
    "\n",
    "\n",
    "The iterative training process in a nutshell: \n",
    "1. The weights are initialized.\n",
    "2. For each input vector, the activations are propagated forward thru the network to give the final output. This is **forward propagation**.\n",
    "3. This final output is compared with the target value to calculate the cost function.\n",
    "4. The above cost is propagated backwards using gradients. This is called **backpropagation**. \n",
    "5. The weights are updated using the gradients calculated above.\n",
    "\n",
    "\n",
    "Many common tasks for supervised machine learning can be formulated as:\n",
    "- Classification\n",
    "- Regression\n",
    "\n",
    "We have studied some binary classification algorithms in the previous session, for which the output variable $y$ is one of the classes - 0 or 1 for the input data $X$ and the function $f$ gives the decision boundary. We will explore regression algorithm in today's session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with more than two classes\n",
    "\n",
    "Suppose you want to use the texts written by 3 different authors to build a neural network that can predict which of those three authors have written an unclassified text.\n",
    "\n",
    "This is a classification problem with three classes. The neural networks we have seen so far only has a single node in the output layer that gives the probability for the positive class for binary (two-class) classification.\n",
    "\n",
    "For a multi-class (more than two classes) classification problem, the number of nodes in the output layer will be equal to the number of classes.\n",
    "\n",
    "<img align=\"left\" src=\"https://drive.google.com/uc?id=1J1JdmJqBOHSqBmEoj1WvYec2vatmIILa\"  >\n",
    "\n",
    "Each node will correspond to one of the classes and it will output the probability for that class.\n",
    "\n",
    "For multiclass classification, we use softmax function instead of sigmoid for output layer and cross-entropy loss instead of log-loss for cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy loss\n",
    "\n",
    "For the binary classification, our label $y$ was simply taking binary integer values $0$ or $1$. For more than two classes, we use one-hot encoding. \n",
    "\n",
    "**One-hot encoding:**   \n",
    "For $k$ classes, we use the k-dimensional vectors to represent each class where only one entry is 1 and others zero. For illustrion, let $k=3$\n",
    "\n",
    "$$ y_1 = (1, 0, 0), \\quad \\quad \\quad y_2 = (0, 1, 0), \\quad \\quad \\quad y_3 = (0, 0, 1)$$\n",
    "\n",
    "\n",
    "The **cross-entropy loss** for the $i^{th}$ sample is given by\n",
    "\n",
    "$$ J^{(i)} = - \\sum_{j=1}^k y^{(i)}_j \\log(p^{(i)}_j)$$\n",
    "\n",
    "The **average cross-entropy loss** across all samples is given by\n",
    "\n",
    "$$ J = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^k y^{(i)}_j \\log(p^{(i)}_j)$$\n",
    "\n",
    "where $y^{(i)}$ is the one-hot encoded label for the $i^{th}$ sample and $p^{(i)}_j$ is the probability of the $j^{th}$ class for the $i^{th}$ sample.  Note that the inner summation will only contain a single non-zero term.\n",
    "\n",
    "\n",
    "Check that cross-entropy loss reduces to log loss for two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax function\n",
    "For a multi-class (more than two classes) problem, the number of nodes in the output layer are equal to the number of classes and we want the probabilities for each class to add up to $1$. One of the simple way to ensure this would be to divide the output for each node by the total sum of the outputs of all the nodes. This is called standard normalization.\n",
    "\n",
    "$$ prob_i = \\frac{z_i}{z_1 + \\dots + z_n} $$\n",
    "\n",
    "The preferred method for multi-class  classification problems is to use softmax function in the output layer. It converts the outputs, say $z_i$'s, into probabilities adding to $1$, by performing standard normalization on the exponentials of the outputs.\n",
    "\n",
    "For each node, the softmax formula is\n",
    "\n",
    "$$softmax(z_i) = \\frac{e^{z_i}}{e^{z_1} + \\dots + e^{z_n}} $$\n",
    "\n",
    "$$ softmax([z_1, \\dots, z_k]) = \\left[\\frac{e^{z_1}}{e^{z_1}+ \\dots + e^{z_k}}, \\dots, \\frac{e^{z_k}}{e^{z_1}+ \\dots + e^{z_k}}\\right]$$\n",
    "\n",
    "The exponentials in the softmax cancels the log in the cross-entropy loss/cost function defined above, causing loss to be linear in $z_i$ and thus, speeding up the backpropagation step.\n",
    "\n",
    "The only assumption for softmax is that the examples cannot belong to two classes at the same time.\n",
    "\n",
    "What do you get for softmax when there are only two classes? Convince yourself that softmax is an extension of sigmoid function for multi-class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression:   \n",
    "- Fitting a curve to determine the impact of feature variables on the target variable.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/8a/Gaussian_kernel_regression.png\" width=\"300\" height=\"350\" />\n",
    "<p style=\"text-align: center;\"> Regression curve </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression: Fitting a ***line*** to determine the impact of feature variables on the target variable.\n",
    "\n",
    "<img align=\"center\" src=\"https://drive.google.com/uc?id=1DRuci3HIlZMBGfYkYSNpgUh_48gmjesl\" width=400 />\n",
    "\n",
    "Note that $X$ consists of independent variables, whereas $y$ target ouput and we want to approximate a function $f: X \\to y$\n",
    "\n",
    "For example, let $x$ be the square feet area of the house and $y$ be the selling price of the house.\n",
    "\n",
    "**Can we use a Multi-Layer Perceptron network to be used for regression? If yes, what will change as compared to the previous one used for binary classification?**\n",
    "\n",
    "<img align=\"center\" src=\"https://drive.google.com/uc?id=1wMVjwS3AsJCfkh-iuIq4MEycUV9yqKuD\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which one of the above two lines is a better fit? Red or Green?\n",
    "* Can there be an even better fit? How do we find out?\n",
    "* What precisely are we looking for in the line that is the best fit?\n",
    "\n",
    "Answer: We are trying to minimize the error in predictions given by the line vs the actual target value.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ed/Residuals_for_Linear_Regression_Fit.png\" width=\"300\" height=\"300\" />\n",
    "\n",
    "The simple linear regression (linear regression with one variable) is formulated as $ y_{pred} = w * x + b $.\n",
    "\n",
    "Finding the best fit line simply means ***finding the optimal values for $w$ and $b$***. For that, we need to quantify the cost function (also known as the error function or the loss function) that we can minimize. \n",
    "\n",
    "* How do we formulate the cost function?\n",
    "* Should we sum up the errors? If not, why?\n",
    "\n",
    "The simple linear regression model uses the mean-squared error (MSE) as the cost function. We square the errors and then take their average.\n",
    "\n",
    "$$ J = \\frac{1}{2 n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})^2 $$\n",
    "\n",
    "Alternatively, we can use Mean-absolute Error (MSE) as the cost function for regression:\n",
    "$$ J = \\frac{1}{n} \\sum_{i=1}^n |y^{(i)} - y_{pred}^{(i)}| $$\n",
    "\n",
    "\n",
    "The cost function is inherently a function of the slope and intercept. This is evident once we substitute $y_{pred}$ with the regression line.\n",
    "\n",
    "$$ J(w, b) = \\frac{1}{2 n} \\sum_{i=1}^n (y^{(i)} - (w * x^{(i)} + b) )^2 $$\n",
    "\n",
    "The [gradient descent algorithm](https://machinelearningmastery.com/gradient-descent-for-machine-learning/), introduced in the last session, can then be used to minimize the above cost function by updating the weights iteratively. \n",
    "\n",
    "Revision: How does the gradient descent algorithm work?\n",
    "* We start with some weights initially\n",
    "* Let the model make prediction $y_{pred}$ for a training example $x$\n",
    "* Compute the cost using the prediction $y_{pred}$ and the actual target $y$ \n",
    "* Compute the gradient of the cost and use it to update the weights\n",
    "* Repeat the above steps for another training example\n",
    "\n",
    "The weights are updated in the direction of the steepest descent of the cost function in each iteration. \n",
    "\n",
    "$$ w := w - \\alpha \\nabla J $$\n",
    "\n",
    "where $\\nabla J$ is the gradient of the cost function $J$ and $\\alpha$ is the learning rate that determines the size of steps that we take descending on the path of gradient.\n",
    "<img align=\"center\"  src=\"https://drive.google.com/uc?id=1K1Ki-VizvgPK88QKCQapedHItBQd8WHr\" width=\"450\" height=\"200\" />\n",
    "<p style=\"text-align: center;\"> Minimizing the cost function using gradient descent </p> \n",
    "\n",
    "To derive the formula for updating weights using gradient descent, we first take the partial derivative, \n",
    "$$ \\frac{\\partial J}{\\partial w} = - \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})   \\ x^{(i)} $$\n",
    "And substitute it in the above equation to get\n",
    "$$ w := w + \\alpha \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})   \\ x^{(i)} $$\n",
    "Similarly,\n",
    "$$ b := b + \\alpha \\frac{1}{n} \\sum_{i=1}^n (y^{(i)} - y_{pred}^{(i)})   $$\n",
    "\n",
    "To summarize, *we defined a cost function to quantify the error in predicting outputs and then we update the weights so as to minimize the cost in the fastest way with the help of gradient descent algorithm*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An illustrative example:**\n",
    "We use a pre-processed sample from the [Boston house prices dataset](https://scikit-learn.org/stable/datasets/index.html#boston-dataset) to predict the house prices based on the number of rooms.\n",
    "* **Input** $x$: Number of rooms\n",
    "* **Target** $y$: Median value of owner-occupied homes\n",
    "\n",
    "<img align=\"center\" src=\"https://drive.google.com/uc?id=1Klx9fs2cAeyHA860s-Nxw9iQVMWjV0cP\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the other notebook titled *Linear regression implementation using gradient descent*, the above gradient descent algorithm is implemented so that the regression line is being updated iteratively and the cost is declining with every iteration.\n",
    "<img align=\"center\" src=\"https://drive.google.com/uc?id=1MjPa-pioGFZ_Wpz38qzYbBlVpDdg1p0P\" width=700 />\n",
    "<img align=\"center\" src=\"https://drive.google.com/uc?id=1QWTVQxzgLL0oqYNcjz1j44JZGKGoS37S\" width=500 />\n",
    "\n",
    "**Exercise**: If you were to implement the above algorithm, what major functions would you need to implement? What would the code look like for the iterative process of weight updates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use [scikit-learn](https://scikit-learn.org/stable/) implementation of the linear regression as demonstrated below while working on problems. The linear regression is often good as a baseline model, and this implementation is fast and require very little code. \n",
    "\n",
    "First we import the function [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and then initialize the regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the regressor using the [`fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) method on the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(-1, 1)\n",
    "lin_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the regression line using the functions written in the python file `linreg.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: 9.559377910614938\n",
      "Intercept -38.406936727358584\n",
      "Cost: 13.165639342962304\n"
     ]
    }
   ],
   "source": [
    "def cost(x, y, w, b, n):\n",
    "    J = 1/(2*n) * np.sum((y - (w*x + b))**2)\n",
    "    return J\n",
    "\n",
    "w = lin_reg.coef_[0]\n",
    "b = lin_reg.intercept_\n",
    "print(\"Weight:\", w)\n",
    "print(\"Intercept\", b)\n",
    "print(\"Cost:\", cost(x, y, w, b, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA600lEQVR4nO3dd5xU1f3/8debZYGlLr0sTbqACootNgR7RY01tphYYqIxxaj5pZiYfNVoYhANit1YolFjiyVKsWALCKJRcelV+tIX2N3P7497V4Zhd+Zumd2Znc/z8ZjHztz6uXPhc++cc+45MjOcc85lj0b1HYBzzrm65YnfOeeyjCd+55zLMp74nXMuy3jid865LOOJ3znnsown/gZK0iZJfaqwfM9wnZxUxhUhjpGSlsR8/p+kkfUXUeaS9EtJ99d3HFUhqbckk9Q4wrIXS3q3LuJqaJJ+uS69SVoAdAZKYyYPMLOWMcs8DCwxs1/Frfd9M3sTwMwWAS1JM2Y2pL5jyFRm9n/1HYNLT37H3zCcbGYtY17L6jugTBbxbrNWb5pqe3vOJeKJv4EKfy73k3QZ8B3gF2FRzkuS/g70BF4Kp/0i/ie2pCmSbpI0VdJGSf+R1CFm+xdKWihpjaRfS1og6ahKYjlR0gxJGyQtlnRjFY7jm+1KulHS05IeDWP6n6QRMct2k/SspFWS5ku6OmbeAZLel1QkabmkuyQ1ifu+fiipECisII7y7+d7khYBk8Lpl0j6QtI6Sa9L6hWzzjGSZktaL+lvkt6S9P1w3sXhd3uHpLXAjZKaSrpd0iJJKyTdIykvXL6DpJfD+NdKekdSo3DedZKWht/JbEmjY76vx2LiOSX8zorC87tn3Pf8c0mzwnifktSsknMSG3uRpHmSvhVOXyxppaSLYpZvE56zVeG/mV/FxJ4THvNqSfOAE+P21UbSA+E5WyrpD6rn4siGwBN/A2dmE4DHgT+FvwZONrMLgEXs/KXwp0pWPw/4LtAJaAL8HEDSYOBvBBeUrkAboCBBGJuBC4F8gv/YP5A0ppqHdArwj3BbLwJ3hTE1Al4CPgljGQ1cI+nYcL1S4CdAB+DgcP6VcdseAxwIDE6w/yOAPYFjw2P4JXA60BF4B3gyjKcD8AxwA9AemA18K25bBwLzCL7fPwK3AgOAYUC/8Dh+Ey77M2BJuJ/O4X5N0kDgR8D+ZtYKOBZYEB+0pAFhbNeE23iF4MLfJGaxs4DjgD2AvYGLE3wPBwKzwmN7guCc7B/GfT5wl6TyosNxBP9G+oTf34UE/64ALgVOAoYDI4Bvx+3nEaAk3O5w4Bjg+wniclGYmb8y+EXwn3wTUBS+ng+nG9AvfP8w8IcK1jsq5nPvcJ3G4ecpwK9i5l8JvBa+/w3wZMy85sD22O0lifmvwB2VzBtJUB+xW5zAjcCbMfMGA1vD9wcCi+K2dQPwUCX7uQb4V8xnA0YliLn8++kTM+1V4HsxnxsBW4BeBMnt/Zh5AhYT1KtAkFQXxc3fDPSNmXYwMD98/3vghfJzGrNMP2AlcBSQGzfvRuCx8P2vgafjYl0KjIz5ns+Pmf8n4J5KvouLgcKYz3uF303nmGlrCC5gOcA2YHDMvMuBKeH7ScAVMfOOKf93SHCB2wbkxcw/F5gcE8e79f1/MBNfXq7YMIyxsJK2ln0d834LOyt/uxEkMQDMbIukNZVtRNKBwC3AUIJfDk2Bf9ZSTM0UFE/1ArpJKoqZn0NwF15+x/sXgrvK5gSJZXrctheTXOwyvYCxkv4cM00Ed+rx35EpprVSBdvqGMY1XVLstsqLNW4jSOT/CedPMLNbzGyOpGvCeUMkvQ781Hav5+kGLIyJp0zSYnb9pRb/3XaLP/gYK2Lebw23GT+tJcEvrCax+w7fl+93l+8pbrleQC6wPOY7aUS08+QS8KKe7FBRF6w16ZZ1OdC9/ENYDt0+wfJPEBTL9DCzNsA9BEmtNi0muDvOj3m1MrMTwvnjgS+B/mbWmqCoJD6GKN9J7DKLgcvj9plnZu+x+3ek2M8VbGs1QbIcErOtNha2zjKzjWb2MzPrA5wM/LS8LN/MnjCzQwkSpREUGcVbFs6PjacHwV1/Kq0GdsTum6B+qXy/y8M4YueVW0xwx98h5jtpbd7Sq8Y88WeHFQTlq8mmRfUMcHJYodcE+B2JE3krYK2ZFUs6gKDuoLZ9BGwIKzrzwkrDoZL2j4lhA7BJ0iDgB7Wwz3uAGyQNgW8qIs8M5/0b2EvSmPAXyQ+BLpVtyMzKgPuAOyR1CrdXUF5HIekkBZX1Co+jFCiVNFDSKElNgWKCi0dpBbt4GjhR0mhJuQR1BtuA92r6JSRiZqXhvv8oqVVY+f1ToLzS+WngakndJbUFro9ZdznwH+DPklpLaiSpr6QjUhlzNvDEnx0eAAaHLTCeD6fdDPwqnPbzqmzMzP4HXEVQobcc2EhQzrytklWuBH4vaSNB/cDTVT+EpDGVEtwJDwPmE9xp3k9QqQhBxfR5Yaz3AU/Vwj7/RXB3/Q9JG4DPgOPDeauBMwnKytcQ1EdMo/LvCOA6YA7wQbi9N4GB4bz+4edNwPvA38xsCkGx2S3h8X5NUFH8ywpinU1Q6TouXPZkgsr97dU7+iq5iqD+Yh7wLsEvwAfDefcBrxNUyn8MPBe37oUERUWfA+sIbjq6pj7khk1hJYlz1Ra23igiKEaZX8/hpKWw1dES4DtmNrm+43HZze/4XbVIOllSc0ktgNuBT6mgGWE2k3SspPywGKa8TuGDeg7LOU/8rtpOJagwXEZQDHGO+c/HeAcDc9lZtDLGzLbWb0jOeVGPc85lHb/jd865LJMRD3B16NDBevfuXd9hOOdcRpk+ffpqM+sYPz0jEn/v3r2ZNm1afYfhnHMZRdLCiqZ7UY9zzmUZT/zOOZdlPPE751yW8cTvnHNZJuWJP+wsa4akl8PPN4Yj6cwMXyck24ZzzrnaUxeten4MfAG0jpl2h5ndXgf7ds65jPT8jKXc9vpslhVtpVt+HtceO5AxwxMNdBddSu/4JXUnGGrv/lTuxznnGpLnZyzlhuc+ZWnRVgxYWrSVG577lOdn1M7wCaku6vkr8AugLG76j8JBnR8M++DejaTLJE2TNG3VqlUpDtM559LHba/PZuuOXYdV2LqjlNten10r209Z4pd0ErDSzOKHtxsP9CXoN3058GcqYGYTzGyEmY3o2HG3B8+cc67BWlZUcV9+lU2vqlTe8R8CnCJpAcGAHaMkPWZmK8ysNGbEoQNSGINzzmWcbvl5VZpeVSlL/GZ2g5l1N7PewDnAJDM7X1Ls6DmnEYxa5JxzLnTtsQPJy83ZZVpebg7XHjuwkjWqpj766vmTpGEEg0IvAC6vhxiccy5tlbfeSVWrnozoj3/EiBHmnbQ551zVSJpuZiPip/uTu845l2U88TvnXJbxxO+cc1nGE79zzmUZT/zOOZdlMmLoRedcdkhlx2RuJ0/8zrm0UN4xWXkfNeUdkwGe/GuZF/U459JCqjsmczt54nfOpYVUd0zmdvLE75xLC6numMztFCnxSzpEUovw/fmS/iKpV2pDc85lk1R3TOZ2inrHPx7YImkfgoFVFgKPpiwq51zWGTO8gJtP34uC/DwEFOTncfPpe3nFbgpEbdVTYmYm6VRgrJk9IOmiVAbmnMs+Y4YXeKKvA1ET/0ZJNwDnA4dLygFyUxeWc865VIla1HM2sA34npl9DRQAt6UsKueccykT6Y4/TPZ/ifm8CC/jd865jJTwjl9SsaS/SmpWVwE555yDlRuKufnVL9hQvKPWt53sjr85wWDouUBxdXYQ1gdMA5aa2UmS2gFPAb0Jhl48y8zWVWfbzjnX0Hy9vph73prLEx8torTMGNGrHUcP7lyr+0iY+M2sDPighvv4MfAF0Dr8fD0w0cxukXR9+Pm6Gu7DOecy2tKirdwzZS5P/XcxZWacsW93rjyyL73at6j1fSVM/JLaADcAY4CO4eSVwAvALWZWlGT97sCJwB+Bn4aTTwVGhu8fAabgid85l6UWr93C+Lfm8s9piwH49n7duXJkP3q0a56yfSYr6nkamASMDCt4kdQFuAj4J3B0kvX/SvDAV6uYaZ3NbDmAmS2X1KmiFSVdBlwG0LNnzyS7cc65zLJozRbunjyHZz9eQiOJc/bvyRUj+1JQB11UJEv8vc3s1tgJ4QXgVkmXJFpR0knASjObLmlkVQMzswnABIARI0ZYVdd3zrl0NH/1Zu6ePId/zVhKTiPxnQODhN+1Td31SZQs8S+U9AvgETNbASCpM3AxsDjJuocAp0g6AWgGtJb0GLBCUtfwbr8rQdGRc841aHNXbeLuSXN4fuZScnMacdHBvbn8iD50bl33jSaTJf6zCSpf3woTPsDXwIvAWYlWNLMbCOoHCO/4f25m50u6jaCo6Jbw7wvVDd4559Jd4YqNjJs0h5dmLaNp40Z879A9uPTwPnRqVX+t5JO16llHUPFam5WvtwBPS/oesAg4sxa37ZxzaeHLrzcwbtIcXvl0OXm5OVx2eB8uPawPHVo2re/Qkj+5K2kQQUucAsCAZcCLZvZF1J2Y2RSC1juY2RpgdDVidc65tPf5sg2Mm1TIq599TcumjfnBEX35/mF9aNeiSX2H9o1kzTmvA84F/gF8FE7uDjwp6R9mdkuK43POuYzw2dL1jJ1YyBufr6BV08ZcPaoflxy6B/nN0yfhl0t2x/89YIiZ7fLMsKS/AP8jKLZxzrms9cniIu6cWMjEL1fSulljrjmqP989ZA/a5KVvB8bJEn8Z0I1g4JVYXcN5zjmXlT5etI47JxYyZfYq2uTl8rOjB3DRIb1p3Sx9E365ZIn/GmCipEJ2Nt/sCfQDfpTCuJxzLi1NW7CWsRMLeadwNW2b5/KL4wZy4cG9adk06vAm9S9Zq57XJA0g6KitABCwBPivmZXWQXzOOZcWPpy3hrETC3lv7hrat2jCDccP4vyDetEigxJ+uaQRm1mZpPnAdsJWPZ70nXPZwMx4f94axr5ZyIfz19KxVVN+deKefOfAXuQ1yUm+gTSVrFXPMOAeoA3Bnb6A7pKKgCvN7ONUB+icc3XNzHh3zmrunFjIfxeso3Prpvz25MGce0BPmuVmbsIvl+yO/2HgcjP7MHaipIOAh4B9UhSXc87VOTPjra9WcefEQj5eVETXNs34/alDOGtEjwaR8MslS/wt4pM+gJl9IKn2O4l2zrl6YGZM+nIld04s5JMl6ynIz+MPY4Zy5ojuNG3ccBJ+uWSJ/1VJ/yYYX7e8VU8P4ELgtVQG5pxzqWZmvPH5Cu6cVMhnSzfQvW0et5y+F6fv250mjROOTJvRkrXquVrS8ezssqG8Vc/dZvZKHcTnnHO1rqzM+M/nXzN24hy+WL6BXu2b86cz9ua0fQvIzWm4Cb9clFY9rwKv1kEszjmXUmVlxquffc24SYV8+fVG+nRowZ/P3IdTh3WjcRYk/HLVboAqaYKZXVabwTjnXCqUlhkvz1rGXZPmULhyE307tmDsOcM4ae9u5DRSfYdX55I152xX2SzghNoPxznnak9JaRkvzVrGuElzmLdqMwM6t+Su84Zz/NCuWZnwyyW7419F0E9P7Ddk4ecKx8p1zrn6tqO0jOdnLOXuyXNYsGYLg7q0Yvx39uXYIV1olMUJv1yyxD8PGG1mi+JnSEo29KJzztWp7SVl/GvGEu6ePJdFa7cwpFtr7r1gP47es7Mn/BjJEv9fgbYEI2XF+1OtR+Occ9WwvaSMZ6Yv4e7Jc1hatJW9u7fh1yeN4Kg9OyF5wo+XrDnn3Qnmjav9cJxzrnLPz1jKba/PZlnRVrrl5/GTo/qzdUcp46fMZdn6Yob1yOcPY4YycmBHT/gJVLlVj6RDgFbA62ZmCZZrBrwNNA3384yZ/VbSjcClBPUHAL/0ZwKcc8k8P2MpNzz3KVt3BH1ELi3ays+fmQXAfr3acssZe3NY/w6e8COIMubuo8CtZvY/SVcAlxOMu3sWcEmCVbcBo8xsk6Rc4F1J5c8D3GFmt9cwdudcFrnt9dnfJP1Y7Vs04ZkrDvaEXwXJmnP2AkYAG8P3lwNXETy9+4qknkCRmW2IXzf8NbAp/Jgbvir9heCcc5XZsr2EpUVbK5y3dvN2T/pVlOxRtZEEXTIfB5wC5AN9gCOAnHB+78pWlpQjaSawEngjpsO3H0maJelBSW0rWfcySdMkTVu1alVFizjnGrhN20oYP2Uuh946udJluuXn1WFEDYMSFNMHC0j3AH0Jkv4zZnZr2DPnq2Z2eKSdSPnAvwh+LawCVhPc/d8EdDWzREVGjBgxwqZNmxZlV865BmBj8Q4efX8h970zj6ItOzhiQEeG9chnwtvzdinuycvN4ebT92LM8IJ6jDZ9SZpuZiPip0ep3L0SOBbYbmYTw2ntgWuj7tzMiiRNAY6LLduXdB/wctTtOOcatvVbd/Dw1AU88O48NhSXcOTAjlw9uj/DewYFA3t0aLFLq55rjx3oSb8aIg29SFwnbeEDXRW17f+GpI7AjjDp5wFHAbdK6mpmy8PFTgM+q1bkzrkGo2jLdh6cuoCHps5nY3EJR+3ZmR+P7s9e3dvsstyY4QWe6GtBKkcJ7go8IimHoC7haTN7WdLfwyEdDVhAUGHsnMtC6zZv54F35/PwewvYtK2E44Z04Uej+jG0oE3ylV21pSzxm9ksYHgF0y9I1T6dawjiH1JqiMUZazZt47535vP39xewZUcpxw/twlWj+rNn19b1HVpWSOUdv3Ouiip6SOmG5z4FaBDJf9XGbdz3zjz+/v5CiktKOWnvblw1qh8DOreq79CySqTEL2kAQWVur9h1zGxUiuJyLitV9JDS1h2l3Pb67IxO/Cs3FHPv2/N4/MOFbC8p45R9uvGjUf3o18kTfn2Iesf/T+Ae4D5g90fnnHO1YlklDylVNj1WOhYRfb2+mHvemssTHy2itMw4dVg3fnRkP/p0bFmvcWW7qIm/xMzGpzQS5xzd8vMqfEI12UNK6VZEtKxoK+OnzOWp/y6mzIzT9y3gh0f2o1f7FnUei9td1MT/kqQrCR7C2lY+0czWpiQq57LUtccO3CWBQ/CQ0rXHDky4XroUES1Zt4W/TZnLP6cFw3WcsW93fnhkP3q0a15nMbjkoib+i8K/sQ9tGUH3Dc65WlKepKtaZFOTIqLasGjNFu6ePIdnP15CI4mz9+/BFUf0pXtbT/jpKFLiN7M9Uh2Icy5QnYeUqltEVFMLVm/mrslz+NeMpeQ0Et85sCdXjOxL1zbef046S9Y75ygzmyTp9Irmm9lzqQnLOVcV1S0iqq55qzZx16Q5PD9zKbk5jbjgoF78YGRfOrdulpL9udqV7I7/CGAScHIF8wzwxO9cGqhuEVFVzVm5kXGT5vDSJ8to0rgR3zt0Dy49vA+dWnnCzyRJe+dMB947p3P1a/bXGxk3qZB/f7qcvNwcLji4F5ce1ocOLZvWd2gugZr0zumcy1KfL9vAuEmFvPrZ17RoksMVR/Tl0sP60K5Fk/oOzdWAJ37n3G4+W7qeOycW8p/PV9CqaWOuHtWPSw7dg/zmnvAbAk/8zrlvfLK4iHGTCnnzi5W0btaYa47qz3e/tQdtmufWd2iuFkXtq+dM4DUz2yjpV8C+wB/M7OOURuecqxMzFq1j7MRCpsxeRZu8XH529AAuOqQ3rZt5wm+Iot7x/9rM/inpUILRuG4HxgMHpiwy51zKTV+4lr++Wcg7hatp2zyXa48dyEXf6k3Lpl4Y0JBFPbvljYNPBMab2QuSbkxNSM65qqhO52wfzlvDnZMKmTpnDe1bNOGG4wdx/kG9aOEJPytEPctLJd3LzuETmxKMquWcq0dV6ZzNzHh/3hrGvlnIh/PX0qFlU3514p6cd2BPmjfxhJ9Nop7ts4DjgNvDMXS7kmSwdUnNgLeBpuF+njGz30pqBzwF9CYYevEsM1tXvfCdy25ROmczM6bOWcPYiV/x3wXr6NSqKb85aTDnHdiTZrk59RG2q2dR++rZImklcChQCJSEfxPZBowys02ScoF3Jb0KnA5MNLNbJF0PXA9cV+0jcC6LJeqczcx4u3A1Y9/8io8XFdGldTN+d8oQzt6/hyf8LBe1Vc9vgRHAQOAhIBd4DDiksnUseCR4U/gxN3wZcCowMpz+CDAFT/zOVUtlnbO1a9GEMX97j08WF1GQn8dNY4Zy1ojuNG3sCd9FL+o5jWDg9I8BzGyZpKRjpknKAaYD/YC7zexDSZ3NbHm4neWSOlWy7mXAZQA9e/aMGKZzO6XjiFS1raLO2QSs2bydvCY53HL6Xpy+b3eaNPYqObdT1MS/3cxMkgFIijSMjpmVAsMk5QP/kjQ0amBmNgGYAEFfPVHXcw7Sb0SqVBkzvIAyM/7w8hes3bIdgPYtm/CLYwdx2r4F5OZ4wne7i5r4nw5b9eRLuhS4hGD83UjCCuEpBBXEKyR1De/2uwIrqxq0c8mky4hUqVRWZrz62ddMeHsea7dsZ48OLfjhkf0YM6wbjT3huwSiVu7eLuloYANBOf9vzOyNROtI6gjsCJN+HmFTUOBFghG9bgn/vlCD+J2rUH2PSJVKpWXGvz9dzriJhRSu3ESfji3469nDOGnvrp7wXSSRG++a2RuSPixfR1K7JGPudgUeCcv5GwFPm9nLkt4n+AXxPWARcGb1w3euYvU1IlUqlZSW8fKs5YybVMjcVZsZ0Lkl484dzgl7dSWnkeo7PJdBorbquRz4PbAVKCOoP0o45q6ZzSKoEI6fvgYYXZ1gnYsqyohUmVL5W1JaxvMzl3H35DnMX72ZQV1a8bfv7MtxQ7rQyBO+q4aod/w/B4aY2epUBuNcbUk2IlUmVP7uKC3juY+XcPfkuSxau4XBXVtzz/n7cczgzp7wXY1ETfxzgS2pDMS52pZo0PJ0rvzdXlLGM9OX8Lcpc1iybit7d2/Dr08awVF7dkLyhO9qLmrivwF4Lyzj31Y+0cyuTklUDUimFCdkm6pW/tbFedxWUsrT05YwfvIclq0vZp8e+dx06lBGDuzoCd/VqqiJ/16CQdc/JSjjdxFkQnFCtqpK5W+qz2PxjlKe+u9ixk+Zy9cbitm3Zz43n7E3h/fv4AnfpUTUxF9iZj9NaSQNUDoXJ2S7KJW/5VJ1HrduL+WJjxZx71tzWblxGwf0bsftZ+7DIf3ae8J3KRU18U8Ou1B4iV2LehI158x6DbkteaZLVvkbq7bP45btJTz+wSLufXseqzdt46A+7Rh7znAO7tu+WttzrqqiJv7zwr83xExL2JzTNcy25A1JosrfWLV1HjdvK+HvHyzkvrfnsWbzdg7p1567Rw3nwD6e8F3divrk7h6pDqQhqkpxgktfNT2PG4t38Oj7C7n/nXms27KDwwd05OpR/RjRu12qQnYuoagPcOUCPwAODydNAe41sx0piqtBqEpxgktf1T2PG4p38MjUBdz/7nzWb93ByIEduXp0f/bt2bYuwnauUgq6zU+ykHQ/QX/6j4STLgBKzez7KYztGyNGjLBp06bVxa6ciyRR8871W3bw4NT5PDh1PhuLSxg9qBNXj+7PPj3y6zdol3UkTTezEfHTo5bx729m+8R8niTpk9oJzbnMUlnzzs3bSvh6QzEPT13Axm0lHDukM1eN6s/Qgjb1HLFzu4qa+Esl9TWzuQCS+gClSdZxrkGqrHnnr57/DATHD+3CVaP6s2fX1vUUoXOJRU381xI06ZxH0EFbL+C7KYvKuTRWWTNOA/5zzeEM6Jx0cDrn6lXUVj0TJfUn6ItfwJdmti3Jas41SJ1bN+PrDcW7TS/Iz/Ok7zJC5P74gf2A3uE6+0jCzB5NSVTOpaEVG4oZP2Uuazbvfs/jzXRdJonanPPvQF9gJjvL9g3wxO8avGVFWxk/ZS5P/XcxpWacsW8B/Tu14uH3FngzXZeRot7xjwAGW5S2n87FyOTeSZes28L4KXN5etpizODb+3XnypH96Nm+OQCXHu4PrrvMFDXxfwZ0AZZH3bCkHgS/CLoQ9Og5wczGSroRuBRYFS76SzN7JXLELmPUR++ktXGhWbx2C3dPnsMz05cgwVkjevCDkX3p3rZ5SmJ2rq4lTPySXiIo0mkFfC7pI3btpO2UBKuXAD8zs48ltQKmSyofoP0OM7u9ZqG7dFfXvZPW9EKzcM1m7po0h+dmLCVH4rwDe3LFEX1r1LdSJv/icQ1Xsjv+aidnM1tO+AvBzDZK+gLwf/FZJEqvlokSY1WTZnUvNPNWbeKuyXN4YeYyGjcSFxzUiyuO6EuXNs2qcri78fEYXLpKmPjN7K3a2Imk3gQDr38IHAL8SNKFwDSCXwXrKljnMuAygJ49e9ZGGK6OJevVMlFiBCqcN23hWiZ/uarCi0FVu0+es3Ijd02aw4ufLKNJ40Z891u9uezwPnRqXbOEX87HY3DpqirNOatFUkvgWeAaM9sgaTxwE0ER0k3An4FL4tczswnABAj66kl1nK72JevVMlFiLH8fP+/xDxZR/o8h/g46avfJX63YyJ0TC/n3p8vJy83h0sP7cOlhfejQsmmNjjeej8fg0lVKE3/Yq+ezwONm9hyAma2ImX8f8HIqY3D1J1mvltVJjPF3ALF30MkuNF8s38C4SYW88unXtGiSwxVH9OXSw/rQrkWTGhxl5Xw8BpeuklXuTjSz0ZJuNbPrqrJhBWPHPQB8YWZ/iZneNSz/BziNoMWQa6ASDXaSLDFWNK8i5ReKyi40/Tq15PK/T+P1/62gZdPGXDWqH5ccsgdtU5Twy/l4DC5dJbvj7yrpCOAUSf8g6K7hG2b2cYJ1DyHovvlTSTPDab8EzpU0jODmbQFwedXDdg1BssQYP68ysXfQsReaWUuKuHPiHN78YgWtmjXmx6P7c8khe9CmeW4tH0nFfDwGl66SJf7fANcD3YG/xM0zYFRlK5rZu8RdKELeZt8B0RLjba/PTnjnX9Ed9IxF67hzYiGTZ6+iTV4uPz16ABcf0pvWzeom4ceKOryjc3Up6kAsvzazm+ogngr5QCzZ7ZBbJlWY/HMk/nzWPt8k1ukL1/LXNwt5p3A1+c1zufSwPlx4cC9a1UPCdy4d1GggFjO7SdIpxAy9aGZeKVsD/mBPdJUVCd18+l6MGV7AR/PXMnbiV0yds4b2LZpw/fGDOP+gXrRsmvJGa85lpKidtN0MHAA8Hk76saRDzOyGlEXWgPmDPTtFuQBWVCT082MG0Ll1M86Z8D4fzFtLh5ZN+X8n7Ml3DupJ8yae8J1LJGpRzyxgmJmVhZ9zgBlmtneK4wMaXlFPZUUXBfl5TL2+0mqTBif+Agi73slXxMyYOmcNd04s5KMFa+nUqilXHNGXcw/oSV6TnLoK3bmMUNMxdwHygbXhex9EtAbS7cGe+ip2qsqTrWbG24WruXNiIdMXrqNL62b87pQhnL1/D5rlesJ3riqiJv6bgRmSJhO01Dkc8GKeakqnB3vqs9gpygXQzJgyexVjJxYyc3ER3do046ZTh3DW/j1o2tgTvnPVEbVy90lJU4D9CRL/dWb2dSoDa8jS6cGe+uxPprILYCOJ3tf/m3YtmtCiSQ6L122lID+P/zttL87Yr8ATvnM1FLmoJ3za9sUUxpI1UvVgT3WKbOqz2KmiCyBAaVjvtHbzdtZthnP278HvTx1Kk8aNUh5TdXgLLZdpvPlDPantB3uqW2RTn8VOsRfApUVbaUQwYk8sA94pXJ3WSd9baLlMk57/m1yVJevpsjLXHjuQvLjK0bosdjp5n25cd/wg+ndquVvSL5fOvVlW93t3rj5FvuOXdCjQ38wektQRaGlm81MXmquK6hbZpLI/mURFICWlZbw0axnjJs1h3qrN9O/UkrbNc1m3Zcdu20nn3izTrYWWc1FEfYDrtwQDrg8EHgJygccIOmJzaaAmRTbxyb/8brUmyb+yIpDSMsOAuyfPYf7qzQzq0oq7z9uX44d24cVPlqVNpXdU6dRCy7moot7xn0YwgtbHAGa2LBxH16WJmrQUilpOXZVKzMqKQH7x7CxKy4zBXVtzz/n7cczgzjRqpF32lUkVpenUQsu5qKIm/u1mZpIMQFKLFMZUZxpSa4yaJM0oTTqrWolZWVFHaZkx4YL9OHpwZ4IhG3Y/jkw6B5l4sXIuauJ/WtK9QL6kSwmGSrwvdWGlnrfG2ClKOXVV2/t3bdOMZeuLd5verU0zjhnSpYYRp5dMu1g5F6lVj5ndDjxDMIziQOA3ZjYulYGlWkNrjVF+IVtatBVj54Xs+RlLk65bWXl07PSolZjFO0p55L0FbN6++wAqebk5/OK4QUnjcc6lVlUe4HoDeCOFsdSphtYao7pP4D4/Yymbt5XsNj2+nDpZJWbxjlKe+HAR97w1l5Ubt7F/77aM6NWOF2YuZfn6Yi8CcS6NRG3Vs5Gd41w3IWjVs9nMWidYpwfwKNCF4LmcCWY2VlI74CmgN8HQi2eZ2brqHkB1NbTWGNW5kFXUOyZA2+a5/PbkIbsk6coqMX88uj8//scMXvpkGWUGTXIa8cORffn5sQORxHXH+x2+c+kmalFPKzNrHb6aAWcAdyVZrQT4mZntCRwE/FDSYIKhHCeaWX9gYvi5ztX3g0u1LUpxTbyKfiUANG/SuMI+8W8+fS8K8vMQQRn+UXt24vcvf84LM4OkD7C9tIwHpy7ghZnLqn0szrnUqtaTu2b2PAnG2w2XWV4+GLuZbQS+AAqAU4FHwsUeAcZUJ4aaik9kBfl5CfuBT3fVuZBV9VfCmOEFvHbNYfz82IFs2LqDl2YtZ1MFxUSZXFfiXDaIWtRzeszHRgQPcyUfwWXn+r0JngP4EOgcdviGmS2X1ClytLWsIbXGqE6zwqoUd20o3sEjUxdw/7vzWb91B412b4m5i2VFWxtUc1nnGpKoI3A9FPOxhKBs/j4zWxlh3ZbAW8Afzew5SUVmlh8zf52Zta1gvcuAywB69uy538KFC5PG6aqmojJ+EVzRC8JEfeTATjz03nwefHc+G4pLOGrPTnyyeD2rNm1LuO22zXMp3lFWpdG1nHO1q7IRuCIl/hrsNBd4GXjdzP4STpsNjAzv9rsSDNyesGC9oQ29WJeS3XWXz19atPWbpF+ucSPROEcU7yjjmMGduXp0f4YWtGGP6/+d8OdeXm4OTRs3omjr7v3uZNvwks7Vp2oNvShpHAmKdMzs6gTrCngA+KI86YdeBC4Cbgn/vpA4dFddUR5SKy/uqmgc4JIyo3GOeOXqwxjcbWcDrsqKiGDnL4WfPDWzwvmZ2lzWuYYkWeXuNGB6glcihwAXAKMkzQxfJxAk/KMlFQJHh59dClTlIbXKEvm2HWW7JH2ovCL5r2cPY+r1oxgzvKBarYycc3Uj4R2/mT2SaH6Sdd8lKDKuyOjqbtdFF6XVzsqNxUx4a95uxTzlKkrUUSqSvfMy59JX1FY9HYHrgMFAs/LpZuaFtWksUaudFRuKueetuTzx4SJ2lJaxX6+2fLp0PdtKdg6HkihRJ2sRFX9xaJOXiwQ/eWomt70+21v4OFePorbjf5ygHf4ewO8IWvX8N0UxuVpSUZFMs8aN6NOhBYf9aTKPvr+QU/bpxqSfjeSZH3yLW8/Yu1afaxgzvICp14/ijrOHsa2kjHVbdlS5HyHnXO2L2pxzupntJ2mWme0dTnvLzI5IeYR4q56aiG2106JJDsUlZQj49n7duXJkP3q2b57yGCqqOAZv4eNcqlWrVU+M8nZ5yyWdCCwDutdWcC519uvVlsMHdOCZ6UvYXlrG2fv34MqRfeneNvUJv1xD6xDPuUwXNfH/QVIb4GfAOKA18JOUReVqbOGazdw9eQ7PfbyURhLnHtCTK47oWy+tahpah3jOZbqoif9DM1sPrAeOTGE8robmr97MXZPm8PzMpTRuJM4/qBdXHNGXD+at4cx73q+X7hO8hY9z6SVq4n9P0nyC7pSfq49ulF1ic1Zu4q5Jhbz4yTKaNG7ERQf35ooj+tCpdbN6H23Mhyd0Lr1E7rJB0gHAOQS9aX4O/MPMHktdaDt55W7lvlqxkXGT5vDyrGU0a5zDBQf34tLD+tCxVdNvlvHKVeeyU00rdzGzj4CPJP0f8BeCLpXrJPG7nWJb6TTLbUTxjjJaNMnhiiP68v1D96B9y6a7reOVq865WJHa8UtqLekiSa8C7wHLgQNSGpnbzfMzlnLds7O+uXsv3hE8bHXCXl247rhBFSZ9qN4gLc65hivqA1yfAMOA35vZADO7zsyS9dXjatGsJUVc/+ysXZ6sLffM9KUJH4ZqaKONOedqJmpRTx9LZf/NrlIzFxcx9s2vmDx7VaXLGCQcVN0rV51zsSIlfk/6dW/6wnWMnVjI21+tIr95LtceO5DHPljI8vXFFS6frLy+IY025pyrmciVu65ufDR/LXdOLOTdOatp16IJ1x03iAsO7kXLpo0pyM/jJ0/NjNyLpnPOVcQTfxowMz6Yt5axE7/ig3lr6dCyCb88YRDnH9SL5k12nqIxwwuYtnAtj3+waJfk7+X1zrmqqEq3zJcCvWPXMbNLUhNWdjAz3pu7hrFvFvLRgrV0bNWUX580mPMO6Elek5wK1/nDmL0Y0audl9c756ot6h3/C8A7wJtAaZJlXaiy8W7NjHcKV3PnxEKmLVxHl9bN+N0pQzh7/x40y6044cfy8nrnXE1ETfzNzey6lEbSwFTUTcL1z87i82Ub+GjBWmYuLqJbm2bcdOoQzhwRLeE751xtiNqO/+VwvNzIJD0oaaWkz2Km3ShpadwYvA1SRePdFpeUMeGdeazauI3/O20vJl87kgsO7u1J3zlXp6Le8f8Y+KWkbQR984uglWfrBOs8DNwFPBo3/Q4zu72qgWaaRM0rp1w7ktycqNdc55yrXVHb8beq6obN7G1JvascUQNQVma0bd6EtVu27zavID/Pk75zrl5Fbs4pqS3Qn10HW3+7Gvv8kaQLgWnAzyrr4lnSZcBlAD179qzGbupeaZnxyqfLGTepkLVbtgc/i2Lme7NL51w6iDrm7vcJinu6AzOBg4D3zSxhn77hHf/LZjY0/NwZWE2QD28CukZpEpru3TKXlhkvz1rGnRMLmbtqM/06teSqUf0oLTX+/MZXad3ssrKWR865zFfTbpl/DOwPfGBmR0oaBPyuqkGY2YqYgO4DXq7qNtJJSWkZL8xcxt2T5zBv9WYGdm7FuHOHc+JeXWnUSACcvl/6Dk1c3wO0NFR+MXXpLmriLzazYklIampmX0qqcpmFpK5mtjz8eBrwWaLl09WO0jL+NWMpd0+ew8I1W9iza2vuOX9fjhnc5ZuEnwkqanm0dUdpwg7fXGJ+MXWZIGriXyIpH3geeEPSOmBZohUkPQmMBDpIWgL8FhgpaRhBUc8C4PLqBF1ftpeU8ezHS/jblDksXruVoQWtmXDBfhw9uDNSkPAz6W7PB2ipfX4xdZkgaque08K3N0qaDLQBXkuyzrkVTH6gauGlh20lpTwzfQl/mzyXpUVb2ad7G248eQijBnX6JuFD5t3tdcvPq3BIRu/wrfr8YuoyQcLEL6m1mW2Q1C5m8qfh35bA2pRFlgaKd5Ty9LTFjJ8yl+XrixneM58/njaUIwZ03CXhl0v3u734XyNHDurIs9OX7hKztzyqutjvtZFEaQUNJvxi6tJJsjv+J4CTgOkExTOx2c6APimKq14V7yjlyY8Wcc9bc1mxYRsjerXl1jP25rD+HSpM+OXS+W6vol8jz05fyhn7FTD5y1UZUTSVjuK/14qSvl9MXbpJmPjN7KTw7x51E0792rq9lMc/XMi9bwfdKhy4RzvuOHsYB/dpnzDhl0vnopPKfo1M/nIVU69P2CrXJVDR9wqQI1Fm5hdTl5aSFfXsm2i+mX1cu+HUj83bSnjsg4Xc9848Vm/azrf6tmfcucM5qE/7Km3n2mMH7nL3B+lzt5fOv0YyWWXfX5kZ8285sY6jcS6aZEU9fw7/NgNGEAy6LmBv4EPg0NSFlnqbtpXw6PsLuP+d+azdvJ3D+nfg6tH92b93u+QrVyCdx7ZN518jyaRzS6lM/l5d9kpW1HMkgKR/AJeZ2afh56HAz1MfXmpsLN7Bw1MX8MDU+RRt2cHIgR25alR/9uvVtkrbqSwhpUtSipXOv0YSSfeWUpn6vbrsFrUd/6DypA9gZp+F7fEzyvqtO3ho6nwefHc+G4pLGD2oE1eN7s+wHvlV3la6J6R46fxrJJF0bymVqd+ry25RE/8Xku4HHiNozXM+8EXKoqplRVu28+C783lo6gI2bivhmMGduXp0f4YWtKn2NtM9IVUkXX+NJJIJdROZ+L267BY18X8X+AFBnz0AbwPjUxJRLVq7eTv3vzOPR95bwObtpRw3pAtXje7HkG7VT/jlMiEhNQRehu5c7Yv65G6xpHuAV8xsdopjqjU3vfw5z89cygl7deWqUf0Y1CXRuDFV4wmpbngZunO1L9KIIJJOIeiO+bXw8zBJL6YwrlpxzVH9ef2aw7n7vH1rNelDkJDy4oZM9IRU+8YML+Dm0/eiID8PEQxkc/Ppe3nRinM1ELWo57fAAcAUADObmQmja/Vq3yJl2/ZKvbrjZejO1a6oib/EzNZHeXo1m3hCcs5loqiJ/zNJ5wE5kvoDVwPvpS4s55xzqRJ11O+rgCHANuBJYANwTYpics45l0JRW/VsAf5f+HLOOZfBknXSlrDljpmdUrvhOOecS7Vkd/wHA4sJinc+ZNf++BOS9CBBX/4rzWxoOK0d8BTQm2DoxbPMbF2Vo3bOOVdtycr4uwC/BIYCY4GjgdVm9paZvZVk3YeB4+KmXQ9MNLP+wMTws3POuTqUMPGbWamZvWZmFwEHAXOAKZKuSrZhM3ub3YdmPBV4JHz/CDCmyhE755yrkaSVu5KaAicC5xIU0dwJPFfN/XU2s+UAZrZcUqcE+70MuAygZ8+e1dydy1Tp3Ae/c5kuWeXuIwTFPK8CvzOzz+okKsDMJgATAEaMGLH7QKauwcq0Lq+dyzTJyvgvAAYQ9Mr5nqQN4WujpA3V2N8KSV0Bwr8rq7EN18Al6vLaOVdzycr4G5lZq/DVOubVysyq0+vZi8BF4fuLgBeqsQ3XwHmX186lVtQnd6tM0pPA+8BASUskfQ+4BThaUiFBC6FbUrV/l7kq69rau7x2rnZE7aunyszs3EpmjU7VPhuSbK7c9D74nUutlCV+V33ZXrnpXV47l1qe+NNQJo7nW9u8y2vnUidlZfyu+rxy0zmXSp7405BXbjrnUskTfxry8Xydc6nkZfxpyCs3nXOp1GATf6Y3h/TKTedcqjTIxJ/tzSGdcy6RBlnG7329OOdc5Rpk4vfmkM45V7kGmfi9OaRzzlWuQSZ+bw7pnHOVa5CVu94c0jnnKtcgEz94c0jnnKtMgyzqcc45VzlP/M45l2U88TvnXJbxxO+cc1nGE79zzmUZmVl9x5CUpFXAwvqOowIdgNX1HUQt8WNJT34s6SeTjqOXmXWMn5gRiT9dSZpmZiPqO47a4MeSnvxY0k9DOA4v6nHOuSzjid8557KMJ/6amVDfAdQiP5b05MeSfjL+OLyM3znnsozf8TvnXJbxxO+cc1nGE38EkhZI+lTSTEnTKpg/UtL6cP5MSb+pjzijkJQv6RlJX0r6QtLBcfMl6U5JcyTNkrRvfcWaSITjyIhzImlgTIwzJW2QdE3cMplyTqIcS0acFwBJP5H0P0mfSXpSUrO4+RlxXirSYLtlToEjzSzRQxvvmNlJdRZN9Y0FXjOzb0tqAjSPm3880D98HQiMD/+mm2THARlwTsxsNjAMQFIOsBT4V9xiGXFOIh4LZMB5kVQAXA0MNrOtkp4GzgEejlksI85LRfyOP4tIag0cDjwAYGbbzawobrFTgUct8AGQL6lr3UaaWMTjyESjgblmFv+UetqfkwpUdiyZpDGQJ6kxwY3Fsrj5mXheAE/8URnwH0nTJV1WyTIHS/pE0quShtRlcFXQB1gFPCRphqT7JbWIW6YAWBzzeUk4LZ1EOQ7IjHMS6xzgyQqmZ8I5iVfZsUAGnBczWwrcDiwClgPrzew/cYtl4nkBPPFHdYiZ7Uvw0+6Hkg6Pm/8xQZ8Y+wDjgOfrOL6oGgP7AuPNbDiwGbg+bhlVsF66tfmNchyZck4ACIurTgH+WdHsCqal2zn5RpJjyYjzIqktwR39HkA3oIWk8+MXq2DVtD0vsTzxR2Bmy8K/KwnKLA+Im7/BzDaF718BciV1qPNAk1sCLDGzD8PPzxAk0PhlesR87s7uP3HrW9LjyKBzUu544GMzW1HBvEw4J7EqPZYMOi9HAfPNbJWZ7QCeA74Vt0ymnZdveOJPQlILSa3K3wPHAJ/FLdNFksL3BxB8r2vqOtZkzOxrYLGkgeGk0cDncYu9CFwYtlg4iOAn7vK6jDOZKMeRKeckxrlUXjSS9uckTqXHkkHnZRFwkKTmYbyjgS/ilsm08/INb9WTXGfgX+G/1cbAE2b2mqQrAMzsHuDbwA8klQBbgXMsfR+Jvgp4PPw5Pg/4btyxvAKcAMwBtgDfra9Ak0h2HBlzTiQ1B44GLo+ZlonnJMqxZMR5MbMPJT1DUDRVAswAJmTqeYnnXTY451yW8aIe55zLMp74nXMuy3jid865LOOJ3znnsownfuecyzKe+F2tkHSaJJM0qL5jqS+SHpb07TrYz5kKeiSdnOp9uYbJE7+rLecC7xL00VJjYe+OWaOKx/s94EozO7KWtueyjCd+V2OSWgKHECSkc8Jpx4dd2ZYvM1LSS+H7YyS9L+ljSf8M1y8f9+A3kt4FzpR0qaT/hh16PRs+HISkvpI+COf9XtKmmP1cG06fJel3lcS7SdIfw+1+IKlzOH2XO/by7YaxvyXpaUlfSbpF0nckfaRgnIa+MZs/StI74XInhevnSLotJq7LY7Y7WdITwKcVxHluuP3PJN0aTvsNcChwj6Tb4pbfZXuSmkl6KNzGDElHhstVNv1iSc9LeknSfEk/kvTTcJkPJLULl7ta0ufhsfwj4T8Ol57MzF/+qtELOB94IHz/HkG/OY0JHntvEU4fHy7XAXg7Zvp1wG/C9wuAX8Rst33M+z8AV4XvXwbODd9fAWwK3x9DMBC2CG5qXgYOryBeA04O3/8J+FX4/mHg2zHLlW93JFAEdAWaEvQz/7tw3o+Bv8as/1q47/4Efbk0Ay6L2UdTYBpB518jCTqY26OCGLuF31/H8LucBIwJ500BRlSwzi7bA34GPBS+HxRur1mC6RcTPIXaKtzveuCKcLk7gGvC98uApuH7/Pr+9+evqr/8jt/VhnOB8ju/fxAk5RKCJHiygv7MTwReAA4CBgNTJc0ELgJ6xWzrqZj3Q8O750+B7wDlXfgezM6eH5+IWf6Y8DWD4FH7QQQJON52gosCwHSgd4Rj/K+ZLTezbcBcoLyL3k/j1n/azMrMrJCgK4lBYUwXhsf7IdA+Jq6PzGx+BfvbH5hiQSdhJcDjBGMQJBO7vUOBvwOY2ZfAQmBAgukAk81so5mtIkj8L1VwnLMIuss4n6A7A5dhvK8eVyOS2gOjCJK0ATmASfoFQRL/IbCWIHFulCTgDTM7t5JNbo55/zDBXe4nki4muKNNGA5ws5ndm2S5HWZW3ldJKTv/H5QQFn+GcTaJWWdbzPuymM9l7Pr/KL4PFAvjusrMXt8lWGkkux5v/LFUR+z2KttGom1HOc4TCS5CpwC/ljQkvDi5DOF3/K6mvk0wClEvM+ttZj2A+QR3lVMIin0uZeed/AfAIZL6QdCpl6QBu28WCIoclkvKJbjjL/cBcEb4PrYy+XXgkpg6gwJJnapwLAuA/cL3pwK5VVi33JmSGoXl/n2A2WFcPwiPA0kDVPHAMbE+BI6Q1CGsqD0XeKuKsbxN+L2F33HPMJ7KpiclqRHQw8wmA78A8oGWVYzL1TNP/K6mzmX3cVWfBc4zs1KCIpXjw7+ERQgXA09KmkWQxCtrAvprggT4BvBlzPRrgJ9K+oig3H19uO3/EBT9vB8WDz1DcPGI6j6CZPsRwdipld2NJzKbIEG/SlA+XgzcT9Bt9MeSPgPuJcmvbQu6970BmAx8QtC//QtVjOVvQE74XTwFXBwWVVU2PYoc4LFw3RnAHdYwhr3MKt47p8s4YeuerWZmks4hqFM4tb7jci5TeBm/y0T7AXeF5fBFwCX1G45zmcXv+J1zLst4Gb9zzmUZT/zOOZdlPPE751yW8cTvnHNZxhO/c85lmf8PkmbwaCRCKTcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the points\n",
    "plt.scatter(x, y)\n",
    "# Plot the regression line\n",
    "x_values = np.linspace(x.min(), x.max(), 100)\n",
    "y_prediction = w*x_values + b\n",
    "plt.plot(x_values, y_prediction);\n",
    "plt.xlabel(\"Average number of rooms\");\n",
    "plt.ylabel(\"Median value of homes in $1000â€™s\");\n",
    "plt.title(\"Fitting a linear regression model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Linear Regression\n",
    "The same formulation and understanding can be extended to linear regression with more than one variable, say $x_1, x_2, \\dots, x_n$, with the equation \n",
    "\n",
    "$$ y_{pred} = b + w_1 * x_1 + w_2 * x_2 + \\cdots + w_n * x_n$$ \n",
    "\n",
    "And we estimate the weights $w_1, w_2, \\dots, w_n$ corresponding to each variable as well as the intercept.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is the simplest example of a neural network with no hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-6f8c9994c154097ada51aa9e113e6515\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Increasing the weight for a variable means it is becoming more impactful in the prediction. \n",
    "* The weights for two variables cannot be compared straightway unless they are in same scale or are normalized to be in the same scale.\n",
    "\n",
    "#### Multivariate regression\n",
    "\n",
    "Linear regression might not work for many datasets, how can we use neural network for curve fitting in general? We can add hidden layers to add complexity/non-linearity to the function that is approximated by the network. Remember, you would not want to use sigmoid, unit step, etc. as activation function for the output layer for regression in general.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/8a/Gaussian_kernel_regression.png\" width=\"300\" height=\"350\" />\n",
    "<p style=\"text-align: center;\"> Regression curve </p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent optimizers\n",
    "\n",
    "* **Stochastic gradient descent**: A single training example is used in each iteration. The gradient (or the derivative of the cost function) is computed for that training example and it is used to update the weights.\n",
    "* **Batch gradient descent**: The entire training set is used in each iteration. The average of the gradients for all the training examples is computed and it is used to update the weights.\n",
    "\n",
    "Notes:\n",
    "* The stochastic converges much faster for larger datasets than the batch gradient descent since the weights are updated a lot more frequently.\n",
    "* For batch gradient descent, the cost function declines consistently with each iteration, whereas for the stochastic gradient descent, the cost fluctuates and declines overall after each epoch.\n",
    "* The stochastic gradient descent cannot make use of the vectorized operations unlike batch gradient descent.\n",
    "\n",
    "\n",
    "**Mini-batch gradient descent** is a mix of the above two and a good compromise. The training set is divided into batches and a single batch is used in each iteration.  The batch sizes of 32, 64 and 128 are often used.\n",
    "\n",
    "In practice, mini-batch is most commonly used, especially for the large datasets. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1634/1*PV-fcUsNlD9EgTIc61h-Ig.png\" width=600 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the learning rate \n",
    "\n",
    "The learning rate ($\\alpha$), that determines the size of the steps in the gradient descent algorithm, is an important hyperparameter that needs to be tuned. \n",
    "* If the learning rate is too low, then it takes too long to converge. \n",
    "* If the learning rate is too high, then it might oscillate and never reach the minima.\n",
    "\n",
    "$$ w := w - \\alpha \\frac{\\partial J}{\\partial w}$$\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*9zqj3nwIEU-L0-9pYitcRA.png\" width=600 />\n",
    "\n",
    "Our intuition tells us that the learning rate $\\alpha$ should be larger at the beginning when are weights are initialized randomly and are far from being optimal and then it can be reduced as the training process proceeds.This is called learning rate schedule or learning rate decay. The two common methods used are linear decay or exponential decay.\n",
    " \n",
    "\n",
    "<img src=\"https://cs231n.github.io/assets/nn3/learningrates.jpeg\" width=300 />\n",
    "\n",
    "\n",
    "The tuning of the learning rate has been an area of research and the adaptive gradient descent algorithms such as Adam, RMSprop, AdaGrad, etc. are developed that usually give better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the number of epochs\n",
    "\n",
    "It is helpful to keep a check on both training and validation errors by printing them out at regular intervals and stop the training process at a suitable time to avoid both underfitting and overfitting to the training set.\n",
    "\n",
    "<img src=\"http://fouryears.eu/wp-content/uploads/2017/12/early_stopping.png\" width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to address overfitting?\n",
    "- Reduce the number of features \n",
    "    - Discard some features\n",
    "    - Dimensionality reduction techniques such PCA, LDA, etc.\n",
    "- Simplify the model (by tuning hyperparameters)\n",
    "- Reducing the number of epochs for training the network\n",
    "- Regularization, Dropout, etc.\n",
    "- Adding more training examples, if possible  \n",
    "<img src=\"https://i.stack.imgur.com/rpqa6.jpg\" width=\"450\" height=\"600\" />\n",
    "\n",
    "In a nutshell, \n",
    "* **To reduce overfitting, reduce complexity.**\n",
    "* **To reduce underfitting, increase complexity.**\n",
    "\n",
    "We will learn the above concepts in more detail in the next session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check out [Tensorflow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=10&networkShape=4,2&seed=0.93390&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
